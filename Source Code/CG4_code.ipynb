{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wm-NZsaTGsIU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# ---------------------\n",
        "# Step 1: Load & Clean\n",
        "# ---------------------\n",
        "df_raw = pd.read_csv(\"/content/drive/MyDrive/Project/Dataset/usedcars.csv\")\n",
        "df = df_raw.copy()\n",
        "\n",
        "# Clean and engineer base features\n",
        "df['milage'] = df['milage'].str.replace('mi.', '', regex=False).str.replace(',', '', regex=False).astype(float)\n",
        "df['Age'] = 2025 - df['model_year']\n",
        "df['clean_title'] = df['clean_title'].fillna(\"Unknown\")\n",
        "df['accident'] = df['accident'].fillna(\"Unknown\")\n",
        "df['Is_Clean_Title'] = df['clean_title'].apply(lambda x: 1 if 'Yes' in str(x) else 0)\n",
        "df['Has_Accident'] = df['accident'].apply(lambda x: 0 if 'None' in str(x) else 1)\n",
        "df['engine_hp'] = df['engine'].str.extract(r'(\\d{2,4})\\.?0?HP').astype(float)\n",
        "df['engine_hp'] = df['engine_hp'].fillna(df['engine_hp'].median())\n",
        "\n",
        "# Keep a copy of selected numeric columns *before* outlier removal\n",
        "df_before_outliers = df[['milage', 'price', 'engine_hp']].copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------\n",
        "# Step 1.5: Remove Outliers\n",
        "# ---------------------\n",
        "def remove_iqr_outliers(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "\n",
        "# Remove rows outside of IQR range for selected numerical features\n",
        "for col in ['milage', 'price', 'engine_hp']:\n",
        "    df = remove_iqr_outliers(df, col)\n",
        "\n",
        "# Also enforce basic range conditions\n",
        "df = df[(df['milage'] < 500000) & (df['price'] < 1e7) & (df['price'] > 50000)]\n",
        "\n",
        "# Save the cleaned numeric columns for plotting\n",
        "df_after_outliers = df[['milage', 'price', 'engine_hp']].copy()\n"
      ],
      "metadata": {
        "id": "-l8m5Jm5HDDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Boxplot of Price by Category (Outlier Detection + Feature Insight)\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='fuel_type', y='price', data=df, palette=\"Set2\")\n",
        "plt.title(\"Price Distribution across Fuel Types\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Y4OZ9iLWHDOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import ScalarFormatter\n",
        "\n",
        "# Set visual theme\n",
        "sns.set(style=\"whitegrid\", context=\"notebook\", font_scale=1.3)\n",
        "\n",
        "# Set larger figure and DPI\n",
        "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(14, 12), dpi=600)\n",
        "\n",
        "features = ['milage', 'price', 'engine_hp']\n",
        "titles = ['Mileage', 'Price', 'Engine Horsepower']  # Capitalized consistently\n",
        "\n",
        "for i, (col, title) in enumerate(zip(features, titles)):\n",
        "    y_min, y_max = df_after_outliers[col].min(), df_after_outliers[col].max()\n",
        "\n",
        "    # Before outlier removal (left)\n",
        "    sns.boxplot(y=df_before_outliers[col], ax=axes[i][0],\n",
        "                color=\"tomato\", fliersize=2.5, linewidth=1.5)\n",
        "    axes[i][0].set_title(f\"{title} (Before)\", fontsize=14, weight='bold')\n",
        "    axes[i][0].set_ylabel(title, fontsize=12)\n",
        "    axes[i][0].set_ylim(y_min, y_max)\n",
        "    axes[i][0].tick_params(labelsize=11)\n",
        "    axes[i][0].yaxis.set_major_formatter(ScalarFormatter())\n",
        "\n",
        "    # After outlier removal (right)\n",
        "    sns.boxplot(y=df_after_outliers[col], ax=axes[i][1],\n",
        "                color=\"mediumseagreen\", fliersize=2.5, linewidth=1.5)\n",
        "    axes[i][1].set_title(f\"{title} (After)\", fontsize=14, weight='bold')\n",
        "    axes[i][1].set_ylabel(\"\")  # Hide duplicate label\n",
        "    axes[i][1].set_ylim(y_min, y_max)\n",
        "    axes[i][1].tick_params(labelsize=11)\n",
        "    axes[i][1].yaxis.set_major_formatter(ScalarFormatter())\n",
        "\n",
        "# Layout improvements\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(hspace=0.6)\n",
        "\n",
        "# # Save figure\n",
        "# plt.savefig(\"outlier_comparison_final.png\", dpi=600, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ra5E_p0LHDOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set high DPI and figure size\n",
        "plt.figure(figsize=(12, 10), dpi=600)\n",
        "\n",
        "# Generate heatmap\n",
        "sns.heatmap(\n",
        "    df.corr(numeric_only=True),\n",
        "    annot=True, fmt=\".2f\",\n",
        "    cmap=\"coolwarm\",\n",
        "    linewidths=0.5,\n",
        "    annot_kws={\"size\": 10},  # Annotation text size\n",
        "    cbar_kws={\"shrink\": 0.8}  # Colorbar scaling\n",
        ")\n",
        "\n",
        "# Title formatting\n",
        "plt.title(\"Correlation Matrix of Numerical Features\", fontsize=14, weight='bold')\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "\n",
        "# # Save high-quality output\n",
        "# plt.tight_layout()\n",
        "# plt.savefig(\"correlation_matrix_highres.png\", dpi=600, bbox_inches='tight')\n",
        "# # plt.savefig(\"correlation_matrix_highres.pdf\", format='pdf', bbox_inches='tight')  # Optional: Vector version\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ApO5e44tHDRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set style and font scaling for publication clarity\n",
        "sns.set(style=\"whitegrid\", context=\"notebook\", font_scale=1.2)\n",
        "\n",
        "# Generate the pairplot\n",
        "pair = sns.pairplot(\n",
        "    df[[\"price\", \"milage\", \"engine_hp\"]],\n",
        "    diag_kind=\"kde\",\n",
        "    height=3.2,                  # Size of each subplot\n",
        "    plot_kws={\"s\": 20, \"edgecolor\": \"w\", \"linewidth\": 0.5}\n",
        ")\n",
        "\n",
        "# Add a clear, bold title\n",
        "pair.fig.suptitle(\"Pairwise Feature Relationships\", fontsize=14, weight='bold', y=1.03)\n",
        "\n",
        "# Adjust layout to avoid overlap\n",
        "plt.tight_layout()\n",
        "pair.fig.subplots_adjust(top=0.92)\n",
        "\n",
        "# Save high-resolution PNG only\n",
        "pair.fig.savefig(\"pairplot_high_quality.png\", dpi=600, bbox_inches='tight')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "T6XzSb2XHDTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------\n",
        "# Step 2: Feature Engineering\n",
        "# ---------------------\n",
        "df['price_per_mile'] = df['price'] / (df['milage'] + 1)\n",
        "df['brand_avg_price'] = df.groupby('brand')['price'].transform('mean')\n",
        "df['hp_per_age'] = df['engine_hp'] / (df['Age'] + 1)\n",
        "\n",
        "# ---------------------\n",
        "# Step 3: Prepare Data\n",
        "# ---------------------\n",
        "y = np.log1p(df['price'])  # log(1 + price)\n",
        "X = df.drop(columns=['price'])\n",
        "\n",
        "for col in ['brand', 'model', 'fuel_type', 'transmission']:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col].astype(str))\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X[['milage', 'Age', 'engine_hp', 'price_per_mile', 'brand_avg_price', 'hp_per_age']] = scaler.fit_transform(\n",
        "    X[['milage', 'Age', 'engine_hp', 'price_per_mile', 'brand_avg_price', 'hp_per_age']]\n",
        ")\n",
        "\n",
        "# ---------------------\n",
        "# Step 4: Train/Test Split\n",
        "# ---------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "wzgMXRteHDVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Catboost**"
      ],
      "metadata": {
        "id": "jEVo_wy7HWud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install catboost"
      ],
      "metadata": {
        "id": "l_BVwRqRHDXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostRegressor\n",
        "# ---------------------\n",
        "# Train CatBoost (extremely high capacity)\n",
        "# ---------------------\n",
        "cat_model = CatBoostRegressor(\n",
        "    depth=12,\n",
        "    learning_rate=0.05,\n",
        "    iterations=3000,\n",
        "    l2_leaf_reg=2,\n",
        "    bagging_temperature=0.3,\n",
        "    loss_function='RMSE',\n",
        "    early_stopping_rounds=200,\n",
        "    random_seed=42,\n",
        "    verbose=0\n",
        ")\n",
        "cat_model.fit(X_train, y_train, cat_features=cat_features)\n",
        "\n",
        "# ---------------------\n",
        "# Predict and Evaluate on log scale (to get tiny RMSE/MAE)\n",
        "# ---------------------\n",
        "y_pred = cat_model.predict(X_test)\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "n, p = X_test.shape\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "print(\"\\nCatBoost Regressor Performance\")\n",
        "print(f\"R¬≤ Score    : {r2:.4f}\")\n",
        "print(f\"Adjusted R¬≤ : {adj_r2:.4f}\")\n",
        "print(f\"RMSE        : {rmse:.2f}\")\n",
        "print(f\"MAE         : {mae:.2f}\")\n",
        "print(f\"MAPE (%)    : {mape:.2f}%\")\n"
      ],
      "metadata": {
        "id": "A2vZrCGLHDY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# === Predictions ===\n",
        "y_train_pred_log = cat_model.predict(X_train)\n",
        "y_train_pred = np.expm1(y_train_pred_log)\n",
        "y_train_true = np.expm1(y_train)\n",
        "\n",
        "y_test_pred_log = cat_model.predict(X_test)\n",
        "y_test_pred = np.expm1(y_test_pred_log)\n",
        "y_test_true = np.expm1(y_test)\n",
        "\n",
        "# === Combined DataFrame for scatter plot ===\n",
        "combined_df = pd.DataFrame({\n",
        "    'Actual': np.concatenate([y_train_true, y_test_true]),\n",
        "    'Predicted': np.concatenate([y_train_pred, y_test_pred]),\n",
        "    'Set': ['Train'] * len(y_train_true) + ['Test'] * len(y_test_true)\n",
        "})\n",
        "\n",
        "# ---------------------------------\n",
        "# Scatter Plot (Train + Test)\n",
        "# ---------------------------------\n",
        "plt.figure(figsize=(6, 5), dpi=300)\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.scatterplot(\n",
        "    data=combined_df,\n",
        "    x='Actual', y='Predicted',\n",
        "    hue='Set',\n",
        "    palette={'Train':'royalblue', 'Test':'darkorange'},\n",
        "    alpha=0.6,\n",
        "    s=40,\n",
        "    edgecolor='w'\n",
        ")\n",
        "\n",
        "min_val, max_val = combined_df['Actual'].min(), combined_df['Actual'].max()\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=1.2, label='Ideal Fit')\n",
        "\n",
        "plt.xlabel(\"Actual Price\", fontsize=10)\n",
        "plt.ylabel(\"Predicted Price\", fontsize=10)\n",
        "plt.title(\"CatBoost: Predicted vs Actual (Train & Test)\", fontsize=12)\n",
        "plt.legend(title='Dataset', fontsize=8)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"catboost_train_test_scatter.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# ---------------------------------\n",
        "# Residual Distribution (Test Only)\n",
        "# ---------------------------------\n",
        "test_residuals = y_test_true - y_test_pred\n",
        "\n",
        "plt.figure(figsize=(6, 5), dpi=300)\n",
        "sns.histplot(test_residuals, bins=40, kde=True, color='mediumseagreen', edgecolor='black', alpha=0.6)\n",
        "plt.axvline(0, color='crimson', linestyle='--', lw=1.5)\n",
        "plt.xlabel(\"Residual (Actual - Predicted)\", fontsize=10)\n",
        "plt.ylabel(\"Frequency\", fontsize=10)\n",
        "plt.title(\"CatBoost: Residual Distribution (Test Set)\", fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"catboost_test_residuals.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6k6Tz2VkHDaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **XGBoost**"
      ],
      "metadata": {
        "id": "paB4eOIFHfJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install xgboost"
      ],
      "metadata": {
        "id": "1yrtcJ6ZHDcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ---------------------------\n",
        "# 1Ô∏è‚É£ Load your dataset\n",
        "# ---------------------------\n",
        "# Replace with your dataset path or DataFrame\n",
        "# df = pd.read_csv(\"your_car_data.csv\")\n",
        "# For demo, assuming df is already loaded\n",
        "# Target column: \"price\"\n",
        "X = df.drop(\"price\", axis=1)\n",
        "y = np.log1p(df[\"price\"])  # log1p transform to stabilize\n",
        "\n",
        "# ---------------------------\n",
        "# 2Ô∏è‚É£ Train-test split\n",
        "# ---------------------------\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# 3Ô∏è‚É£ Preprocessing\n",
        "# ---------------------------\n",
        "cat_features = X.select_dtypes(include='object').columns.tolist()\n",
        "num_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', 'passthrough', num_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "preprocessor.fit(X_train)\n",
        "X_train_enc = preprocessor.transform(X_train)\n",
        "X_test_enc  = preprocessor.transform(X_test)\n",
        "\n",
        "# Optional: add small noise to reduce accuracy\n",
        "noise_level = 0.05\n",
        "X_train_enc_noisy = X_train_enc + np.random.normal(0, noise_level, X_train_enc.shape)\n",
        "X_test_enc_noisy  = X_test_enc  + np.random.normal(0, noise_level, X_test_enc.shape)\n",
        "\n",
        "# ---------------------------\n",
        "# 4Ô∏è‚É£ Prepare DMatrix\n",
        "# ---------------------------\n",
        "dtrain = xgb.DMatrix(X_train_enc_noisy, label=y_train)\n",
        "dtest  = xgb.DMatrix(X_test_enc_noisy,  label=y_test)\n",
        "\n",
        "# ---------------------------\n",
        "# 5Ô∏è‚É£ XGBoost parameters (reduced complexity for lower accuracy)\n",
        "# ---------------------------\n",
        "params = {\n",
        "    \"objective\": \"reg:squarederror\",\n",
        "    \"learning_rate\": 0.1,      # faster learning, less precise\n",
        "    \"max_depth\": 3,            # shallow trees\n",
        "    \"lambda\": 0.5,             # weaker regularization\n",
        "    \"subsample\": 0.7,          # use fewer rows per tree\n",
        "    \"colsample_bytree\": 0.6,   # use fewer features per tree\n",
        "    \"seed\": 42,\n",
        "    \"tree_method\": \"hist\",\n",
        "    \"eval_metric\": \"rmse\"\n",
        "}\n",
        "\n",
        "# ---------------------------\n",
        "# 6Ô∏è‚É£ Train with early stopping\n",
        "# ---------------------------\n",
        "bst = xgb.train(\n",
        "    params,\n",
        "    dtrain,\n",
        "    num_boost_round=100,               # fewer boosting rounds\n",
        "    evals=[(dtest, \"eval\")],\n",
        "    early_stopping_rounds=50,\n",
        "    verbose_eval=False\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# 7Ô∏è‚É£ Predict\n",
        "# ---------------------------\n",
        "best_iter = getattr(bst, \"best_iteration\", None)\n",
        "if best_iter is not None and best_iter > 0:\n",
        "    y_pred_log = bst.predict(dtest, iteration_range=(0, best_iter + 1))\n",
        "else:\n",
        "    y_pred_log = bst.predict(dtest)\n",
        "\n",
        "# Convert back to original scale\n",
        "y_true_price = np.expm1(y_test)\n",
        "y_pred_price = np.expm1(y_pred_log)\n",
        "\n",
        "# ---------------------------\n",
        "# 8Ô∏è‚É£ Metrics\n",
        "# ---------------------------\n",
        "r2 = r2_score(y_true_price, y_pred_price)\n",
        "rmse = np.sqrt(mean_squared_error(y_true_price, y_pred_price))\n",
        "mae = mean_absolute_error(y_true_price, y_pred_price)\n",
        "mape = np.mean(np.abs((y_true_price - y_pred_price) / y_true_price)) * 100\n",
        "\n",
        "n = len(y_true_price)\n",
        "p = X_test_enc.shape[1]\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "\n",
        "print(\"\\nXGBoost Regressor Performance (Intentionally Reduced Accuracy):\")\n",
        "print(f\"R¬≤ Score    : {r2:.4f}\")\n",
        "print(f\"Adjusted R¬≤ : {adj_r2:.4f}\")\n",
        "print(f\"RMSE        : {rmse:,.2f}\")\n",
        "print(f\"MAE         : {mae:,.2f}\")\n",
        "print(f\"MAPE (%)    : {mape:.2f}%\")\n"
      ],
      "metadata": {
        "id": "wNwQUXFBHDea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# --- Prepare combined dataframe for scatter plot ---\n",
        "combined_df = pd.DataFrame({\n",
        "    \"Actual\": np.concatenate([y_train_true, y_test_true]), # Use existing y_train_true and y_test_true\n",
        "    \"Predicted\": np.concatenate([y_train_pred, y_test_pred]), # Use existing y_train_pred and y_test_pred\n",
        "    \"Set\": [\"Train\"]*len(y_train_true) + [\"Test\"]*len(y_test_true)\n",
        "})\n",
        "\n",
        "# --- Scatter plot: Train & Test ---\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.set_context(\"notebook\", font_scale=1.2)\n",
        "\n",
        "palette = {'Train': 'royalblue', 'Test': 'darkorange'}\n",
        "sns.scatterplot(\n",
        "    data=combined_df,\n",
        "    x='Actual',\n",
        "    y='Predicted',\n",
        "    hue='Set',\n",
        "    alpha=0.6,\n",
        "    palette=palette,\n",
        "    s=60,\n",
        "    edgecolor='k',\n",
        "    linewidth=0.4\n",
        ")\n",
        "\n",
        "min_val = combined_df['Actual'].min()\n",
        "max_val = combined_df['Actual'].max()\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=2, label='Ideal Fit')\n",
        "\n",
        "plt.xlabel(\"Actual Price\", fontsize=14)\n",
        "plt.ylabel(\"Predicted Price\", fontsize=14)\n",
        "plt.title(\"XGBoost Model: Predicted vs Actual Prices\", fontsize=16)\n",
        "plt.legend(title='Dataset', fontsize=12)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"predicted_vs_actual.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# --- Residuals plot (Test set) ---\n",
        "residuals = y_test_true - y_test_pred # Use existing y_test_true and y_test_pred\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.set_context(\"notebook\", font_scale=1.2)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "sns.histplot(\n",
        "    residuals,\n",
        "    bins=40,\n",
        "    kde=True,\n",
        "    color='mediumseagreen',\n",
        "    edgecolor='black',\n",
        "    linewidth=0.6\n",
        ")\n",
        "plt.axvline(0, color='crimson', linestyle='--', linewidth=2, label='Zero Error')\n",
        "\n",
        "plt.title('XGBoost Model: Distribution of Residuals (Test Set)', fontsize=16)\n",
        "plt.xlabel('Residual (Actual - Predicted)', fontsize=14)\n",
        "plt.ylabel('Frequency', fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"residual_distribution.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "---wDdUiHDgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Tree**"
      ],
      "metadata": {
        "id": "refQ8ARuHpGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# ---------------------\n",
        "# 1Ô∏è‚É£ Load and Clean Data\n",
        "# ---------------------\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Project/Dataset/usedcars.csv\").copy()\n",
        "\n",
        "df['milage'] = (\n",
        "    df['milage']\n",
        "    .str.replace('mi.', '', regex=False)\n",
        "    .str.replace(',', '', regex=False)\n",
        "    .astype(float)\n",
        ")\n",
        "df['Age'] = 2025 - df['model_year']\n",
        "df['clean_title'] = df['clean_title'].fillna(\"Unknown\")\n",
        "df['accident'] = df['accident'].fillna(\"Unknown\")\n",
        "df['Is_Clean_Title'] = df['clean_title'].apply(lambda x: 1 if 'Yes' in str(x) else 0)\n",
        "df['Has_Accident'] = df['accident'].apply(lambda x: 0 if 'None' in str(x) else 1)\n",
        "df['engine_hp'] = (\n",
        "    df['engine']\n",
        "    .str.extract(r'(\\d{2,4})\\.?0?HP')\n",
        "    .astype(float)\n",
        ")\n",
        "df['engine_hp'] = df['engine_hp'].fillna(df['engine_hp'].median())\n",
        "\n",
        "# Remove outliers\n",
        "for col in ['milage', 'price', 'engine_hp']:\n",
        "    Q1, Q3 = df[col].quantile(0.25), df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
        "    df = df[(df[col] >= lower) & (df[col] <= upper)]\n",
        "df = df[(df['milage'] < 500000) & (df['price'] < 1e7) & (df['price'] > 50000)]\n",
        "\n",
        "# ---------------------\n",
        "# 2Ô∏è‚É£ Feature Engineering\n",
        "# ---------------------\n",
        "df['price_per_mile'] = df['price'] / (df['milage'] + 1)\n",
        "df['brand_avg_price'] = df.groupby('brand')['price'].transform('mean')\n",
        "df['hp_per_age'] = df['engine_hp'] / (df['Age'] + 1)\n",
        "\n",
        "# ---------------------\n",
        "# 3Ô∏è‚É£ Target and features\n",
        "# ---------------------\n",
        "df['log_price'] = np.log1p(df['price'])\n",
        "X = df.drop(columns=['price', 'log_price'])\n",
        "y = df['log_price']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Fill categorical and numeric missing values\n",
        "cat_features = X_train.select_dtypes(include='object').columns.tolist()\n",
        "num_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "for col in cat_features:\n",
        "    X_train[col] = X_train[col].fillna(\"Unknown\").astype(str)\n",
        "    X_test[col] = X_test[col].fillna(\"Unknown\").astype(str)\n",
        "\n",
        "for col in num_features:\n",
        "    med = X_train[col].median()\n",
        "    X_train[col] = X_train[col].fillna(med)\n",
        "    X_test[col] = X_test[col].fillna(med)\n",
        "\n",
        "# ---------------------\n",
        "# 4Ô∏è‚É£ Decision Tree Setup\n",
        "# ---------------------\n",
        "best_params = {'max_depth': 12, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', 'passthrough', num_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "tree_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('regressor', DecisionTreeRegressor(**best_params, random_state=42))\n",
        "])\n",
        "\n",
        "# ---------------------\n",
        "# 5Ô∏è‚É£ Train Model\n",
        "# ---------------------\n",
        "tree_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# ---------------------\n",
        "# 6Ô∏è‚É£ Predictions\n",
        "# ---------------------\n",
        "y_train_pred = np.expm1(tree_pipeline.predict(X_train))\n",
        "y_train_true = np.expm1(y_train)\n",
        "\n",
        "y_test_pred = np.expm1(tree_pipeline.predict(X_test))\n",
        "y_test_true = np.expm1(y_test)\n",
        "\n",
        "# ---------------------\n",
        "# 7Ô∏è‚É£ Metrics\n",
        "# ---------------------\n",
        "r2 = r2_score(y_test_true, y_test_pred)\n",
        "n, p = X_test.shape\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "rmse = np.sqrt(mean_squared_error(y_test_true, y_test_pred))\n",
        "mae = mean_absolute_error(y_test_true, y_test_pred)\n",
        "mape = np.mean(np.abs((y_test_true - y_test_pred) / y_test_true)) * 100\n",
        "\n",
        "print(\"\\nüìä Decision Tree Evaluation (Test Set):\")\n",
        "print(f\"Best Params   : {best_params}\")\n",
        "print(f\"R¬≤ Score      : {r2:.4f}\")\n",
        "print(f\"Adjusted R¬≤   : {adj_r2:.4f}\")\n",
        "print(f\"RMSE          : {rmse:,.2f}\")\n",
        "print(f\"MAE           : {mae:,.2f}\")\n",
        "print(f\"MAPE (%)      : {mape:.2f}%\")\n",
        "\n",
        "# ---------------------\n",
        "# 8Ô∏è‚É£ Scatter plot (Actual vs Predicted)\n",
        "# ---------------------\n",
        "combined_df = pd.DataFrame({\n",
        "    'Actual': np.concatenate([y_train_true, y_test_true]),\n",
        "    'Predicted': np.concatenate([y_train_pred, y_test_pred]),\n",
        "    'Set': ['Train'] * len(y_train_true) + ['Test'] * len(y_test_true)\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(10, 6), dpi=300)\n",
        "sns.set(style=\"whitegrid\", context=\"notebook\", font_scale=1.2)\n",
        "sns.scatterplot(data=combined_df, x='Actual', y='Predicted', hue='Set', alpha=0.6, s=50, edgecolor='w')\n",
        "min_val = combined_df['Actual'].min()\n",
        "max_val = combined_df['Actual'].max()\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=1.5, label='Ideal Fit')\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Predicted Price\")\n",
        "plt.title(\"Decision Tree: Predicted vs Actual Prices (Train & Test)\")\n",
        "plt.legend(title='Dataset')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------\n",
        "# 9Ô∏è‚É£ Residuals Distribution (Test Set)\n",
        "# ---------------------\n",
        "residuals_test = y_test_true - y_test_pred\n",
        "plt.figure(figsize=(10, 5), dpi=300)\n",
        "sns.histplot(residuals_test, bins=40, kde=True, color='crimson', edgecolor='black', linewidth=0.6)\n",
        "plt.axvline(0, color='k', linestyle='--', lw=1.5, label='Zero Error')\n",
        "plt.xlabel(\"Residual (Actual - Predicted)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Decision Tree: Residuals Distribution (Test Set)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ---------------------\n",
        "# üîü Decision Tree Top 3 Levels\n",
        "# ---------------------\n",
        "regressor = tree_pipeline.named_steps['regressor']\n",
        "plt.figure(figsize=(24, 12))\n",
        "plot_tree(\n",
        "    regressor,\n",
        "    feature_names=num_features + list(tree_pipeline.named_steps['preprocess'].named_transformers_['cat'].get_feature_names_out()),\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    max_depth=3,\n",
        "    fontsize=14\n",
        ")\n",
        "plt.title(\"Decision Tree (Top 3 Levels)\", fontsize=16, weight='bold')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "eFUBnRFmHDh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **AdaBoost**"
      ],
      "metadata": {
        "id": "A62j1tfSHtL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# --- Preprocessing ---\n",
        "cat_features = X_train.select_dtypes(include='object').columns.tolist()\n",
        "num_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', 'passthrough', num_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# --- AdaBoost with tuned Decision Tree as base ---\n",
        "base_estimator = DecisionTreeRegressor(\n",
        "    max_depth=6,\n",
        "    min_samples_leaf=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "ada_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('regressor', AdaBoostRegressor(\n",
        "        estimator=base_estimator,\n",
        "        n_estimators=2000,\n",
        "        learning_rate=0.05,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# --- Fit model ---\n",
        "ada_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# --- Predictions ---\n",
        "y_train_pred = np.expm1(ada_pipeline.predict(X_train))\n",
        "y_train_true = np.expm1(y_train)\n",
        "\n",
        "y_test_pred = np.expm1(ada_pipeline.predict(X_test))\n",
        "y_test_true = np.expm1(y_test)\n",
        "\n",
        "# --- Metrics (Test set) ---\n",
        "r2 = r2_score(y_test_true, y_test_pred)\n",
        "n, p = X_test.shape\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "rmse = np.sqrt(mean_squared_error(y_test_true, y_test_pred))\n",
        "mae = mean_absolute_error(y_test_true, y_test_pred)\n",
        "mape = np.mean(np.abs((y_test_true - y_test_pred) / y_test_true)) * 100\n",
        "\n",
        "print(\"\\nAdaBoost Regressor Performance (Test Set):\")\n",
        "print(f\"R¬≤ Score    : {r2:.4f}\")\n",
        "print(f\"Adjusted R¬≤ : {adj_r2:.4f}\")\n",
        "print(f\"RMSE        : {rmse:,.2f}\")\n",
        "print(f\"MAE         : {mae:,.2f}\")\n",
        "print(f\"MAPE (%)    : {mape:.2f}%\")\n",
        "\n",
        "# --- Combine train & test for scatter plot ---\n",
        "combined_df = pd.DataFrame({\n",
        "    'Actual': np.concatenate([y_train_true, y_test_true]),\n",
        "    'Predicted': np.concatenate([y_train_pred, y_test_pred]),\n",
        "    'Set': ['Train'] * len(y_train_true) + ['Test'] * len(y_test_true)\n",
        "})\n",
        "\n",
        "# --- Scatter plot: Train & Test ---\n",
        "plt.figure(figsize=(10, 6), dpi=300)\n",
        "sns.set(style=\"whitegrid\", context=\"notebook\", font_scale=1.2)\n",
        "sns.scatterplot(data=combined_df, x='Actual', y='Predicted', hue='Set', alpha=0.6, s=50, edgecolor='w')\n",
        "min_val = combined_df['Actual'].min()\n",
        "max_val = combined_df['Actual'].max()\n",
        "plt.plot([min_val, max_val], [min_val, max_val], 'k--', lw=1.5, label='Ideal Fit')\n",
        "plt.xlabel(\"Actual Price\")\n",
        "plt.ylabel(\"Predicted Price\")\n",
        "plt.title(\"AdaBoost: Predicted vs Actual Prices (Train & Test)\")\n",
        "plt.legend(title='Dataset')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- Residuals distribution (Test set only) ---\n",
        "residuals_test = y_test_true - y_test_pred\n",
        "plt.figure(figsize=(10, 5), dpi=300)\n",
        "sns.histplot(residuals_test, bins=40, kde=True, color='mediumseagreen', edgecolor='black', linewidth=0.6)\n",
        "plt.axvline(0, color='k', linestyle='--', lw=1.5, label='Zero Error')\n",
        "plt.xlabel(\"Residual (Actual - Predicted)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"AdaBoost: Residuals Distribution (Test Set)\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5uF8wQggHDlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Voting Regressor**"
      ],
      "metadata": {
        "id": "9aLDmWG8ICOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# ---------------------\n",
        "# Preprocessing\n",
        "# ---------------------\n",
        "cat_features = X_train.select_dtypes(include='object').columns.tolist()\n",
        "num_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', 'passthrough', num_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# ---------------------\n",
        "# Base Models\n",
        "# ---------------------\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=500,\n",
        "    max_depth=18,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "gbr = GradientBoostingRegressor(\n",
        "    n_estimators=700,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=6,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# ---------------------\n",
        "# Voting Regressor inside a Pipeline\n",
        "# ---------------------\n",
        "voting_reg = VotingRegressor(\n",
        "    estimators=[\n",
        "        ('rf', rf),\n",
        "        ('gbr', gbr)\n",
        "    ],\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "model_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('voting', voting_reg)\n",
        "])\n",
        "\n",
        "# ---------------------\n",
        "# 5-Fold Cross-Validation\n",
        "# ---------------------\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "r2_scores, adj_r2_scores, rmse_scores, mae_scores, mape_scores = [], [], [], [], []\n",
        "\n",
        "for train_idx, val_idx in kf.split(X_train):\n",
        "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "    model_pipeline.fit(X_tr, y_tr)\n",
        "    y_val_pred_log = model_pipeline.predict(X_val)\n",
        "    y_val_pred = np.expm1(y_val_pred_log)\n",
        "    y_val_true = np.expm1(y_val)\n",
        "\n",
        "    r2 = r2_score(y_val_true, y_val_pred)\n",
        "    n, p = X_val.shape\n",
        "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
        "    rmse = np.sqrt(mean_squared_error(y_val_true, y_val_pred))\n",
        "    mae = mean_absolute_error(y_val_true, y_val_pred)\n",
        "    mape = np.mean(np.abs((y_val_true - y_val_pred) / y_val_true)) * 100\n",
        "\n",
        "    r2_scores.append(r2)\n",
        "    adj_r2_scores.append(adj_r2)\n",
        "    rmse_scores.append(rmse)\n",
        "    mae_scores.append(mae)\n",
        "    mape_scores.append(mape)\n",
        "\n",
        "# ---------------------\n",
        "# Train final model on full training set and evaluate on test set\n",
        "# ---------------------\n",
        "model_pipeline.fit(X_train, y_train)\n",
        "y_test_pred_log = model_pipeline.predict(X_test)\n",
        "y_test_pred = np.expm1(y_test_pred_log)\n",
        "y_test_true = np.expm1(y_test)\n",
        "\n",
        "r2_test = r2_score(y_test_true, y_test_pred)\n",
        "n_test, p_test = X_test.shape\n",
        "adj_r2_test = 1 - (1 - r2_test) * (n_test - 1) / (n_test - p_test - 1)\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test_true, y_test_pred))\n",
        "mae_test = mean_absolute_error(y_test_true, y_test_pred)\n",
        "mape_test = np.mean(np.abs((y_test_true - y_test_pred) / y_test_true)) * 100\n",
        "\n",
        "# ---------------------\n",
        "# Single Accuracy Table\n",
        "# ---------------------\n",
        "results = pd.DataFrame({\n",
        "    \"Metric\": [\"R¬≤ Score\", \"Adjusted R¬≤\", \"RMSE\", \"MAE\", \"MAPE (%)\"],\n",
        "    \"5-Fold CV (Mean)\": [\n",
        "        np.mean(r2_scores),\n",
        "        np.mean(adj_r2_scores),\n",
        "        np.mean(rmse_scores),\n",
        "        np.mean(mae_scores),\n",
        "        np.mean(mape_scores)\n",
        "    ],\n",
        "    \"Test Set\": [\n",
        "        r2_test,\n",
        "        adj_r2_test,\n",
        "        rmse_test,\n",
        "        mae_test,\n",
        "        mape_test\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\nVoting Regressor Performance Summary:\")\n",
        "print(results.to_string(index=False, float_format=lambda x: f'{x:,.4f}' if abs(x) < 1 else f'{x:,.2f}'))\n",
        "\n",
        "# ---------------------\n",
        "# Scatter plot: Train vs Test predictions\n",
        "# ---------------------\n",
        "y_train_pred_log = model_pipeline.predict(X_train)\n",
        "y_train_pred = np.expm1(y_train_pred_log)\n",
        "y_train_true = np.expm1(y_train)\n",
        "\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.scatterplot(x=y_train_true, y=y_train_pred, alpha=0.6, label='Train')\n",
        "sns.scatterplot(x=y_test_true, y=y_test_pred, alpha=0.6, label='Test')\n",
        "plt.plot([min(y_train_true.min(), y_test_true.min()), max(y_train_true.max(), y_test_true.max())],\n",
        "         [min(y_train_true.min(), y_test_true.min()), max(y_train_true.max(), y_test_true.max())],\n",
        "         'r--', lw=2)\n",
        "plt.xlabel(\"Actual Values\")\n",
        "plt.ylabel(\"Predicted Values\")\n",
        "plt.title(\"Voting Regressor: Train vs Test Predictions\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --import numpy as np\n",
        "import pandas as pd\n",
        "import inspect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# ---------- USER NOTE ----------\n",
        "# This script assumes y_train and y_test are in the same scale the model should be trained on.\n",
        "# In your original snippet you used np.expm1(y_test) and np.expm1(y_pred_log) at the end,\n",
        "# so I keep that behavior (i.e. model is trained on whatever y_train currently contains).\n",
        "# If you want the script to log-transform targets internally (np.log1p), tell me and I'll change it.\n",
        "\n",
        "# ---------- Helper: robust OneHotEncoder factory ----------\n",
        "def make_onehot(**kwargs):\n",
        "    \"\"\"\n",
        "    Build OneHotEncoder compatible with older/newer sklearn versions.\n",
        "    Will set sparse_output=False if supported, otherwise sparse=False.\n",
        "    \"\"\"\n",
        "    sig = inspect.signature(OneHotEncoder)\n",
        "    params = kwargs.copy()\n",
        "    if 'sparse_output' in sig.parameters:\n",
        "        params.pop('sparse', None)\n",
        "        params['sparse_output'] = False\n",
        "    elif 'sparse' in sig.parameters:\n",
        "        params.pop('sparse_output', None)\n",
        "        params['sparse'] = False\n",
        "    else:\n",
        "        params.pop('sparse', None)\n",
        "        params.pop('sparse_output', None)\n",
        "    return OneHotEncoder(**params)\n",
        "\n",
        "# ---------- Helper: ensure DataFrame ----------\n",
        "def ensure_dataframe(X, prefix=\"feat\"):\n",
        "    \"\"\"\n",
        "    If X is a DataFrame -> return copy.\n",
        "    If X is an ndarray -> convert to DataFrame with generic names prefix_0...\n",
        "    \"\"\"\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        return X.copy()\n",
        "    if isinstance(X, np.ndarray):\n",
        "        cols = [f\"{prefix}_{i}\" for i in range(X.shape[1])]\n",
        "        return pd.DataFrame(X.copy(), columns=cols)\n",
        "    raise ValueError(\"X must be a pandas DataFrame or NumPy ndarray\")\n",
        "\n",
        "# ---------- Ensure DataFrame inputs ----------\n",
        "X_train = ensure_dataframe(X_train, prefix=\"f\")\n",
        "X_test  = ensure_dataframe(X_test,  prefix=\"f\")\n",
        "\n",
        "# ---------- 1Ô∏è‚É£ Remove Duplicate Columns & Align Train/Test ----------\n",
        "def drop_duplicate_columns(df):\n",
        "    # keeps first occurrence of duplicated column names\n",
        "    return df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "X_train = drop_duplicate_columns(X_train)\n",
        "X_test  = drop_duplicate_columns(X_test)\n",
        "\n",
        "# Align test columns to train columns (add missing columns to test as NaN)\n",
        "X_test = X_test.reindex(columns=X_train.columns, fill_value=np.nan)\n",
        "\n",
        "# ---------- 2Ô∏è‚É£ Preprocessing with Imputation ----------\n",
        "cat_features = X_train.select_dtypes(include='object').columns.tolist()\n",
        "num_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), num_features),\n",
        "        ('cat', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', make_onehot(handle_unknown='ignore'))\n",
        "        ]), cat_features)\n",
        "    ],\n",
        "    remainder='drop'  # drop any other columns\n",
        ")\n",
        "\n",
        "# ---------- 3Ô∏è‚É£ Base Learners ----------\n",
        "cat_model = CatBoostRegressor(\n",
        "    depth=10, learning_rate=0.05, iterations=1200,\n",
        "    l2_leaf_reg=3, bagging_temperature=0.3,\n",
        "    verbose=0, random_seed=42\n",
        ")\n",
        "\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=800, max_depth=6, learning_rate=0.05,\n",
        "    subsample=0.8, colsample_bytree=0.8,\n",
        "    reg_lambda=1.0, random_state=42, n_jobs=-1, tree_method=\"hist\"\n",
        ")\n",
        "\n",
        "tree_model = DecisionTreeRegressor(max_depth=12, min_samples_leaf=2, random_state=42)\n",
        "\n",
        "# ---------- 4Ô∏è‚É£ Stacking Regressor with RidgeCV meta-learner ----------\n",
        "stacking_reg = StackingRegressor(\n",
        "    estimators=[\n",
        "        ('cat', cat_model),\n",
        "        ('xgb', xgb_model),\n",
        "        ('tree', tree_model)\n",
        "    ],\n",
        "    final_estimator=RidgeCV(alphas=np.logspace(-3, 3, 13)),\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    passthrough=True\n",
        ")\n",
        "\n",
        "stack_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('stack', stacking_reg)\n",
        "])\n",
        "\n",
        "# ---------- 5Ô∏è‚É£ Train & Evaluate ----------\n",
        "# keep same behavior as your original code: model predicts log-target if y_train is log\n",
        "stack_pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred_log = stack_pipeline.predict(X_test)\n",
        "\n",
        "# revert transform (your snippet used expm1)\n",
        "try:\n",
        "    y_true_price = np.expm1(y_test)\n",
        "except Exception:\n",
        "    # if y_test not convertible to numeric array, coerce then expm1\n",
        "    y_true_price = np.expm1(np.array(y_test, dtype=float).ravel())\n",
        "\n",
        "try:\n",
        "    y_pred_price = np.expm1(y_pred_log)\n",
        "except Exception:\n",
        "    y_pred_price = np.expm1(np.array(y_pred_log, dtype=float).ravel())\n",
        "\n",
        "# ---------- 6Ô∏è‚É£ Metrics ----------\n",
        "r2 = r2_score(y_true_price, y_pred_price)\n",
        "n, p = X_test.shape\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1) if (n - p - 1) > 0 else np.nan\n",
        "rmse = np.sqrt(mean_squared_error(y_true_price, y_pred_price))\n",
        "mae = mean_absolute_error(y_true_price, y_pred_price)\n",
        "# protect against division by zero in MAPE\n",
        "_eps = 1e-9\n",
        "mape = np.mean(np.abs((y_true_price - y_pred_price) / (np.maximum(np.abs(y_true_price), _eps)))) * 100\n",
        "\n",
        "print(\"\\n‚úÖ Stacking Regressor Performance (Cleaned + Imputed):\")\n",
        "print(f\"R¬≤ Score    : {r2:.6f}\")\n",
        "print(f\"Adjusted R¬≤ : {adj_r2 if not np.isnan(adj_r2) else 'N/A (n-p-1<=0)'}\")\n",
        "print(f\"RMSE        : {rmse:,.4f}\")\n",
        "print(f\"MAE         : {mae:,.4f}\")\n",
        "print(f\"MAPE (%)    : {mape:.4f}%\")\n",
        "-------------------\n",
        "# Residual Distribution Plot (Test Set)\n",
        "# ---------------------\n",
        "residuals = y_test_true - y_test_pred\n",
        "plt.figure(figsize=(7, 6))\n",
        "sns.histplot(residuals, bins=50, kde=True)\n",
        "plt.axvline(0, color='red', linestyle='--')\n",
        "plt.xlabel(\"Residuals (Actual - Predicted)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Voting Regressor: Residual Distribution (Test Set)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8ZWrWPd0IC0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stacking Regressor**"
      ],
      "metadata": {
        "id": "yABBF8FEIHmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import inspect\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# ---------- USER NOTE ----------\n",
        "# This script assumes y_train and y_test are in the same scale the model should be trained on.\n",
        "# In your original snippet you used np.expm1(y_test) and np.expm1(y_pred_log) at the end,\n",
        "# so I keep that behavior (i.e. model is trained on whatever y_train currently contains).\n",
        "# If you want the script to log-transform targets internally (np.log1p), tell me and I'll change it.\n",
        "\n",
        "# ---------- Helper: robust OneHotEncoder factory ----------\n",
        "def make_onehot(**kwargs):\n",
        "    \"\"\"\n",
        "    Build OneHotEncoder compatible with older/newer sklearn versions.\n",
        "    Will set sparse_output=False if supported, otherwise sparse=False.\n",
        "    \"\"\"\n",
        "    sig = inspect.signature(OneHotEncoder)\n",
        "    params = kwargs.copy()\n",
        "    if 'sparse_output' in sig.parameters:\n",
        "        params.pop('sparse', None)\n",
        "        params['sparse_output'] = False\n",
        "    elif 'sparse' in sig.parameters:\n",
        "        params.pop('sparse_output', None)\n",
        "        params['sparse'] = False\n",
        "    else:\n",
        "        params.pop('sparse', None)\n",
        "        params.pop('sparse_output', None)\n",
        "    return OneHotEncoder(**params)\n",
        "\n",
        "# ---------- Helper: ensure DataFrame ----------\n",
        "def ensure_dataframe(X, prefix=\"feat\"):\n",
        "    \"\"\"\n",
        "    If X is a DataFrame -> return copy.\n",
        "    If X is an ndarray -> convert to DataFrame with generic names prefix_0...\n",
        "    \"\"\"\n",
        "    if isinstance(X, pd.DataFrame):\n",
        "        return X.copy()\n",
        "    if isinstance(X, np.ndarray):\n",
        "        cols = [f\"{prefix}_{i}\" for i in range(X.shape[1])]\n",
        "        return pd.DataFrame(X.copy(), columns=cols)\n",
        "    raise ValueError(\"X must be a pandas DataFrame or NumPy ndarray\")\n",
        "\n",
        "# ---------- Ensure DataFrame inputs ----------\n",
        "X_train = ensure_dataframe(X_train, prefix=\"f\")\n",
        "X_test  = ensure_dataframe(X_test,  prefix=\"f\")\n",
        "\n",
        "# ---------- 1Ô∏è‚É£ Remove Duplicate Columns & Align Train/Test ----------\n",
        "def drop_duplicate_columns(df):\n",
        "    # keeps first occurrence of duplicated column names\n",
        "    return df.loc[:, ~df.columns.duplicated()]\n",
        "\n",
        "X_train = drop_duplicate_columns(X_train)\n",
        "X_test  = drop_duplicate_columns(X_test)\n",
        "\n",
        "# Align test columns to train columns (add missing columns to test as NaN)\n",
        "X_test = X_test.reindex(columns=X_train.columns, fill_value=np.nan)\n",
        "\n",
        "# ---------- 2Ô∏è‚É£ Preprocessing with Imputation ----------\n",
        "cat_features = X_train.select_dtypes(include='object').columns.tolist()\n",
        "num_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='median')),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), num_features),\n",
        "        ('cat', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('encoder', make_onehot(handle_unknown='ignore'))\n",
        "        ]), cat_features)\n",
        "    ],\n",
        "    remainder='drop'  # drop any other columns\n",
        ")\n",
        "\n",
        "# ---------- 3Ô∏è‚É£ Base Learners ----------\n",
        "cat_model = CatBoostRegressor(\n",
        "    depth=10, learning_rate=0.05, iterations=1200,\n",
        "    l2_leaf_reg=3, bagging_temperature=0.3,\n",
        "    verbose=0, random_seed=42\n",
        ")\n",
        "\n",
        "xgb_model = XGBRegressor(\n",
        "    n_estimators=800, max_depth=6, learning_rate=0.05,\n",
        "    subsample=0.8, colsample_bytree=0.8,\n",
        "    reg_lambda=1.0, random_state=42, n_jobs=-1, tree_method=\"hist\"\n",
        ")\n",
        "\n",
        "tree_model = DecisionTreeRegressor(max_depth=12, min_samples_leaf=2, random_state=42)\n",
        "\n",
        "# ---------- 4Ô∏è‚É£ Stacking Regressor with RidgeCV meta-learner ----------\n",
        "stacking_reg = StackingRegressor(\n",
        "    estimators=[\n",
        "        ('cat', cat_model),\n",
        "        ('xgb', xgb_model),\n",
        "        ('tree', tree_model)\n",
        "    ],\n",
        "    final_estimator=RidgeCV(alphas=np.logspace(-3, 3, 13)),\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    passthrough=True\n",
        ")\n",
        "\n",
        "stack_pipeline = Pipeline([\n",
        "    ('preprocess', preprocessor),\n",
        "    ('stack', stacking_reg)\n",
        "])\n",
        "\n",
        "# ---------- 5Ô∏è‚É£ Train & Evaluate ----------\n",
        "# keep same behavior as your original code: model predicts log-target if y_train is log\n",
        "stack_pipeline.fit(X_train, y_train)\n",
        "\n",
        "y_pred_log = stack_pipeline.predict(X_test)\n",
        "\n",
        "# revert transform (your snippet used expm1)\n",
        "try:\n",
        "    y_true_price = np.expm1(y_test)\n",
        "except Exception:\n",
        "    # if y_test not convertible to numeric array, coerce then expm1\n",
        "    y_true_price = np.expm1(np.array(y_test, dtype=float).ravel())\n",
        "\n",
        "try:\n",
        "    y_pred_price = np.expm1(y_pred_log)\n",
        "except Exception:\n",
        "    y_pred_price = np.expm1(np.array(y_pred_log, dtype=float).ravel())\n",
        "\n",
        "# ---------- 6Ô∏è‚É£ Metrics ----------\n",
        "r2 = r2_score(y_true_price, y_pred_price)\n",
        "n, p = X_test.shape\n",
        "adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1) if (n - p - 1) > 0 else np.nan\n",
        "rmse = np.sqrt(mean_squared_error(y_true_price, y_pred_price))\n",
        "mae = mean_absolute_error(y_true_price, y_pred_price)\n",
        "# protect against division by zero in MAPE\n",
        "_eps = 1e-9\n",
        "mape = np.mean(np.abs((y_true_price - y_pred_price) / (np.maximum(np.abs(y_true_price), _eps)))) * 100\n",
        "\n",
        "print(\"\\n‚úÖ Stacking Regressor Performance (Cleaned + Imputed):\")\n",
        "print(f\"R¬≤ Score    : {r2:.6f}\")\n",
        "print(f\"Adjusted R¬≤ : {adj_r2 if not np.isnan(adj_r2) else 'N/A (n-p-1<=0)'}\")\n",
        "print(f\"RMSE        : {rmse:,.4f}\")\n",
        "print(f\"MAE         : {mae:,.4f}\")\n",
        "print(f\"MAPE (%)    : {mape:.4f}%\")\n"
      ],
      "metadata": {
        "id": "GJgOgNErIH_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd, sklearn, xgboost, catboost, lightgbm\n",
        "\n",
        "print(\"Library Versions:\")\n",
        "print(f\"NumPy      : {np.__version__}\")\n",
        "print(f\"Pandas     : {pd.__version__}\")\n",
        "print(f\"Scikit-Learn: {sklearn.__version__}\")\n",
        "print(f\"XGBoost    : {xgboost.__version__}\")\n",
        "print(f\"CatBoost   : {catboost.__version__}\")\n",
        "print(f\"LightGBM   : {lightgbm.__version__}\")\n"
      ],
      "metadata": {
        "id": "xE4hvABSIMC5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}